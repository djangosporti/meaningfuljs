The missing link of computer meaning
====================================

Introduction
------------

Everything is meaningful. But what's about computer technologies? As for today meaning is rather in human minds but not in computers. Any information is just bytes for machines and algorithms and acquires meaning only when interpreted by humans. Some semantic approaches assume metadata would convert data into meaning but would they? Real meaning makes us understand information but would metadata make computers "understand" data? No, at least in the current implementation. Understanding is an intimate act of human mind, which is based on understanding basic principles of the Nature and abstraction. And metadata does not give such understanding. Metadata is just a description of structures, which generalize data. It is just small subset of meaning. And any generalization is just one way of doman describing.

Typical example of modern semantic approaches is an imaginary situation when a human requests an intelligent agent with some complex question in natural language. We cannot deny natural language processing works better than ten years ago but with many restrictions. We cannot deny some complex searches work for _some_ domains. But semantic approaches promise to cover all cases and all domains, don't they? And if they cannot fulfill promises, maybe we need to try something different? What's if we will have a tool, which can operate with both natural language and computer data? What's if this tool will be simple enough to be understood by ordinary users? What's if the entire computer ecosystem will absorb principles of meaning and will become more meaningful than is now? Of course, for that we need to clearly understand what is meaning and introduce meaning into mainstream computer technologies and approaches. We need paradigm shift of not only data/metadata/meaning but also of software design, implementation, operating systems, and user interfaces.

Meaning in computer
-------------------

What is common between programming, software design, documentation, databases, files, user interface, natural language, hypertext, pictures, videos, folksonomies, Internet search, and Semantic Web? First, they may relate to computers. Second, they have "e" character, consist of less than three words, and the first letter is not less than "d" and not more than "u". Third, they all are not living things (at least according with modern definition of "living"), not eatable, not drinkable, and do not fly (well...). This may sound funny but this is good example to explain the nature of abstractions, which are detached from reality. They allow us to think flexibly, we potentially can and we do find common traits between arbitrary things. Seriously, though these conceptions may appear too different but they all deal with meaning in computer:

* Programming is the most efficient way (as for computer implementation) of meaning expressing. At the same time, it requires significant effort and you cannot create meaning as easily as with other tools. This creates problems if software architecture and/or used technologies do not fit intentions of software users anymore: a part or the whole software is re-written, which again implies significant effort.
* Software design expresses meaning with abstractions, which are slightly different comparing with implementation. Because software design and implementation are maintained by different tools and often by different people, there are numerous problems of synchronization between them.
* The story of synchronization issues continues when implementation is documented (which expresses meaning in human readable form). Now we have synchronization issues between design and implementation, plus documentation.
* Databases store identifiers, which acquire meaning in software (and later in human mind).
* File system is another way of keeping information (and meaning) but which requires additional set of identifiers (file path) to know how to retrieve it. It is assumed file path (and file name) should be meaningful (at least for creators) but it is not always so. What is worse, file path creates one way of classification of information in files, which restricts its usage in terms of meaning.
* User interface helps to express meaning in human readable (or audible) way. But this meaning isolated (to some degree) from one of software design, implementation, documentation, and databases. Graphical user interface makes us think in terms of UI paths (with clicks/opens), which is not user friendly enough.
* Natural language is sophisticated and flexible tool for expressing meaning (developed through millennia) but at the same time, it is ambiguous and context dependent. We are on the way of its efficient usage in computer industry but this way may be long.
* Hypertext is the way of linking documents/resources (and different representations of meaning). It uses natural language (so incorporates its benefits and shortcomings) and hypertext references. The latter, in own turn, is ambiguous and may imply (a) an association between terms, (b) a reference to different parts of the same site, (c) a reference to another source of information (like in books), (d) a brief explanation for a term, etc. Such reference always implies one-to-one relation (reference-to-document), though, any word, sentence, or paragraph may be associated with many pieces of information (eg, `the Sun` may refer to many definitions or relevant pages about the Sun).
* Pictures are worth a thousand words but they may lack explicit explanation of meaning. Also they are not very good in depicting abstract conceptions unless schemes are used (which, in own turn, has another restriction: if we have too many elements, schemes are difficult to create and use). Though we cannot describe everything with pictures and schemes but this type of representation may be extremely useful in certain cases.
* Videos combine advantages and flaws of natural language and pictures, and have more limitations: it is hard to refer to videos and from videos (there are some ways but they have limited applicability), sometimes videos are time consuming (as some instructions are faster to read or see at pictures than to watch the whole video, especially, if interested fragments are scattered through video).
* Folksonomies help to categorize meaning with tags/keywords but they are difficult to use if number of tags is too high and do not use relations between tags/keywords.
* Internet search works on the base of relevancy, which produces good enough results with more or less precise names and titles but fails with more abstract words linked with generalizing relations (eg, `the brightest source of light on our planet`). It assumes words inside query are related to each other but it could be not so (for example, `the closest star to the Earth`, `the closest star to France` work more or less as expected but `the closest star to Paris` suddenly gives results with hotels and railway stations for rather `star Paris` query). Usage of relevancy finally means we receive the most probable or the most popular results, which won't be sufficient in all cases.
* Semantic Web originally claimed to be "a Web of data" so it is closer rather to programming and databases (but object model of Semantic Web assumes more flexible classes and open world model). Many conceptions of it are at least questionable. It assumes unique identification for things but is it possible to identify _any_ thing? In our conversation, we may refer to arbitrary scope of things (we may consider them with as many details as we want or as much generalized as we want, we may combine different things in arbitrary groups, etc), moreover, sometimes we consider a group of things only once (and creation of a unique identifier may be useless). So, do we need a unique identifier for each arbitrary and sometimes temporary scope of meaning? Further, classes/objects of Semantic Web still use abbreviations and some conventions, which are rather specific for software engineering. But if we want to work with meaning, we should be ready to have meaning everywhere (including names themselves) and conceptions should have at least some link with common understanding of it. Semantic Web widely uses triples, which reflect _usual_ order of natural language sentence but it does not mean such model will be efficient in all cases: in fact, we can imagine a lot of situations, when we need either nouns (for only static description) or mostly verbs (for explanation of behavior only). Moreover, due to dualism of space-time our notions of noun/adjective/verb/adverbs are quite intricate, that is, this division is conditional to some degree.

As you can see, advantages and shortcomings of these items balance between ad-hoc efficiency and flexibility and bias of specific approach always leads to problems of synchronization between different ways of meaning representation. You may hear about similar balance under different guises: universality and uniqueness, abstraction and specification, generalization and particularity, consistency and completeness. For example, in terms of data, we can exemplify this as a balance between unstructured data (as text) and structured one (databases, applications, Semantic Web). The problem of text is we can extract meaning of it only basing on statistical approaches (like Internet search), which is not reliable enough. The problem of structured data is you cannot make everything consistent because there are infinite variants of consistency and consistent data creates additional volume of information, which after some time should be re-ordered. Possibly we need a bridge between these opposites but at first we need to understand how meaning encompass both sides.

To understand what meaning is we need to consider factors, which influences it the most: (1) what we cognize (outer world, representations of it, representations of representations, etc), (2) how we cognize (through reducing, abstracting, etc). The former implies that meaning tries to imitate real space-time with object-actions and their characteristics. The latter implies any meaning is balancing between generalizing and specifying. Finally, it only somehow relates to an original and consists of abstractions only. You can easily imagine `Venus` may be a part of a sci-fi book, where it could be detached from what it is in real life namely because `Venus` is an abstraction. However, of course, if we underline `Venus` corresponds to real Venus, then such detachment is unacceptable and will be considered as "false".

Thus, the task of meaning recognition is bidirectional: (1) we identify the most unique abstractions, which make this meaning different from others, (2) we generalize abstractions to find what meaning is similar to. Namely therefore meaning in natural language is a complex of unique identifiers linked with generalized relations: we cannot have unique identifiers for everything and we cannot use too general descriptions, which matches too many things. On the other hand, structured data mostly uses unique identifiers with an implied set of generalized relations: such approach is extremely efficient for restricted domain but has high complexity with widening of considered scope. Is a compromise possible?

Semantic markup
---------------

As we shown above, there is a gap between programming and software design, between Semantic Web and Internet search, between ad-hoc efficiency of computer applications and flexibility of natural language, between expected data structures and arbitrariness of text. Can we fill this gap? Can we have balanced approach, which will work with both sides? One of such attempts is natural language processing but it is far from maturity and general availability. What is an alternative? Look closer at data and natural language: they are mutually dependent. Natural language consists of data (but there are no good tools to extract it), data has natural language inside (decoded in field values, field and class names, etc). What's if we could combine both? Meaning is always present in natural language words, phrases, and sentences, and if there are no automatic tools to extract it, then we should use manual ones. Meaning is always present in data values, variable and class names, but it is implied and there are no tools to work with it. What's if we can have a tool, which can be used in programming languages, software design, databases, Internet search, Semantic Web, and be natural language itself? Is it realistic?

What's if we will use text markup based on natural language (as a convenient form of meaning for everyone) with minimal syntax, which allows to make meaning more explicit? To minimize interference with natural language we use curly brackets (as rarely used in natural language) or, alternatively, square brackets. Minimal goal for such markup is identifier isolation, which can help in simple cases of ambiguity like `New York buildings`, which may be marked up as:

`New York {} buildings` (that is "buildings of New York") or `New {} York {} buildings` (that is, "New buildings of York").

Empty markup (`{}`) just draws boundaries between identifiers and makes meaning more evident (but, of course, boundaries may be drawn with punctuation itself). The next step is referring to identifiers with `#` sign and short names (which is not useful per se but may prove its efficiency when applied to long identifiers or their complexes):

`{#1} Venus {/#1} is the second planet from the Sun, orbiting it every 224.7 Earth days.`

All said above may be applied to identifiers, which express more general relations (like `is`):

`Venus {} is {} the second planet {} from the Sun, orbiting it every 224.7 Earth days.`

As well the most generalized relations can be used inside markup:

`Venus {is #1} is the second {#1} planet {/#1} from the Sun, orbiting it every 224.7 Earth days.`

Simpler (or even implicit) markup is possible too (syntax is experimental):

`Venus {planet} orbits {} the Sun {star}` (here `is` relation implied)
`Venus {is "planet"} orbits {} the Sun {is "star"}` (more specific identifiers are quoted)
`Venus {"planet"} orbits {} the Sun {"star"}` (combination of the first two variants)
`Venus {is { planet }} orbits {} the Sun {is { star }}` (alternatively, we can use curly brackets for markup inside markup)

Also markup can be complementary to natural language text (for example, we can specify in headers, all occurrences of `Venus` in text are `Venus {is} planet` unless stated otherwise). Such alternative approach limits interference with text though could expose synchronization issues in the case of frequent modifications. Therefore, we may turn to HTML, where meaning can be applied with, say, attributes:

`<span meaning="is instance of #1">Venus</span> is <span meaning="#1">planet</span>`

Such syntax potentially implies markup can be multi-language (but we have to have strict correspondence between the most general relations and relaxed one between identifiers):

`Vénus {est #1} est une des quatre {#1} planètes {/#1} telluriques du Système solaire`

Markup relations
----------------

To facilitate usage of the most general relations we need to comply with two points: (a) a set of such relations should be restricted, (b) these relations should preferably coincide with natural language words/phrases. But the question is which relations are the most general ones? First, it is identifiers/references (which correspond to different text units). Second, it is undefined relation (which in natural language corresponds to punctuation marks, whitespaces, etc). Whitespaces can express different relations: in `Venus orbits` the whitespace links a noun and a verb in one object-action, in `Venus chemistry` the whitespace links two nouns into the intersection of two meanings, in `orbit around` the whitespace links object-action with spatial relation, whereas in `the Sun`, the whitespace is just a boundary of words. For marking up an undefined relation with greater weight than whitespace we can use `^` sign: 

`Venus {^ #1} is the second {#1} planet {/#1} from the Sun, orbiting it every 224.7 Earth days.`

Now, we must return to factors, which influence meaning. They outline following undefined `is` and undefined `has`/`of` relations. Undefined `is` corresponds to similarity between things and expresses how we cognize the world (by comparing similarities/differences). Undefined `has` and opposite `of` correspond to composability and express what we cognize (that is, real space-time). Undefined `has`/`of` may be applied to real objects (then it concerns parts and properties like in `Venus has mountains`), real actions (then it concerns parts of action in time and properties of actions like manner, etc), complex of object-actions (then it concerns participants, their parts and properties, etc), abstractions (then it concerns abstract "parts" and "properties", which mixed for abstraction more flexibly), etc.

The next level is defined `is`/`has`/`of` relations. First, relations, which match undefined `is` relation:

* `is similar` as in `Venus is similar to the Sun`. The fact of similarity stated without any specifics.
* `is the same` as `it {is the same} Venus` in `It orbits`. Precise referencing is possible when equality rules are defined (eg, numbers, references to references/identifiers like pronouns, etc).
* `is type of` / `is instance of` (alternative `is class of` or short `type` and `class` can be considered too) as in `Venus is a planet`. A type/class is a group of similar things (instances) with at least one similarity criteria (which can be implicit). Classification is quite arbitrary as we can combine things in almost infinite number of ways. For example, `food` may be divided into `eatable` and `drinkable` (but this division is not strict), to `boiled` or `stewed`, to `vegetarian` or `not vegetarian`, etc, etc. Classes/types can be declarations of common properties/attributes in some cases.
* `abstracts` / `specifies` as in `Venus is the second planet from the Sun`. A relation between less specified meaning and more specified one (or vice versa) and which are similar to each other.

Second, ones, which match undefined `has`/`of` relation:

* `has part`/`is part of` as in `the Solar System has Venus`. Explicit statement of structural inclusivity/composability, which helps to consider things partially or jointly. It is required as each "level" of inclusivity/composability have different item types/classes (thus, we can compare the Solar system with other planet systems and its planets with planets of other planet system separately).
* `has property` / `is property of` as in `Venus has color`. Used for characteristics, which are inherent to something but not a part of it (in terms of structural inclusivity). For real space-time properties usually means characteristics of things, which do not relate to space-time itself.
* `has value` / `is value of` as in `Venus color is grey`. A value expresses specifics of characteristics.

As you can see, we use light hierarchy of relations (unknown - undefined `is`/`has`/`of` - their defined counterparts), which gives more flexibility with relations. Similar flexibility applies to "values", "properties", "instances", and other relations, which have quite flexible attachment to things. For example, `color` can be both "property" and "type", whereas `yellow` can be "value", "instance", or "type" (if we require higher level of specification for yellow color). That is, these relation types are rather "roles" of things and conceptions, because abstraction allows to use them flexibly. 

Please also note, `is` used in `is part of` and other relations but this is only language convention. `Venus is part of the Solar System` has no indication of similarity. That is, if in `Venus is instance of planet` we can remove `instance of` and `Venus is planet` will make sense but `Venus is the Solar System` won't.

Further, we need to consider conditional relation (which may relate to undefined `has` one):

* `has condition` / `is condition of` (as a variant, it may be expressed with just `if`). Conditions mostly used as similarity criteria for types and modal actions though sometimes conditions are used without type creation. For example, `4GB of RAM` is a condition in `OS_like_OS can be run on computer with 4 GB of RAM` and is a conditional "part" of "computer memory". We can express it like `computer {has} memory {has condition} 4 GB of RAM` (we omit "more than" relation for brevity). If we want to know if a computer can run on OS_like_OS, you need to compare `my computer {has} memory {has} 2 GB of RAM` against the previous statement. As you can see, conditions usually compared with other `has` relations and may produce a new type of "computers, which can be run on OS_like_OS" (but you should decide if declaration of this type is expedient in your case or not).

And two relations which potentially relates to `has` one (they express rather composability of object-action dualism and cause-effect chains):
  
* `does` / `is done` / `done by` / `does what` as in `Venus orbits`. Relations for expressing object-action dualism and actors involved, if an object-action implies several ones. Please note, it is another language convention that a verb can have only two kinds of participants (subject, object), but, generally speaking, it is not quite so (as well as a choice between subject/object is quite anthropocentric thing). Therefore, instead of `done by` and `does what` you may use different kinds of `has` relations (or maybe we need specialized relations like `has action`/`has verb`/`has object`/`has predicate`/`has participant`, etc).
* `causes` / `caused by` as in `Earth causes lunar eclipse`. A relation, which expresses a cause-effect order of actions in time. In natural language sometimes we use `has cause`, which is rather abstract kind of `has` relation applying to an action.

Finally, we should consider other nevertheless important relations (and which are not restricted with listed below):
  
* `behind`, `under`, `place`, etc. All spatial relations, which usually relates rather to real things and which are used for some abstract ones (a book, a movie, or an application).
* `after`, `before`, `the past`, `future`, `date`, `event`, etc. All temporal relations, which can be used for both real and abstract time.
* Mathematical (numbers, operations, comparison operators, `units of`, etc).
* Quantitative (`all`, `some`, `any`, `none`)
* Logical, set ones (`true`, `false`, `not`, `and`, `or`)

Above mentioned relations are basic one, we intentionally don't go deeper to have cleaner picture. We don't consider mathematical, quantitative, logical, and set relations as they researched thoroughly in programming. The remained point, which we need to clarify is a subject as it is one of major points, which affects meaning itself. For example, `John thinks {} Venus {} is {} planet {@John}`. Syntax is simple: `@` sign marks a "container" of meaning and is similar to namespaces in XML or programming languages. But it is not for having unique identifiers but rather to underline subjectivity of meaning (but which can use the same identifiers).

Why is this approach so different?
----------------------------------

Meaning recognition is not only about identifiers and relations but rather determining meaning scope with the help of unique abstractions (to find how meaning is different from others) and generalized abstractions (to find what meaning is similar to), which makes a scope narrower or wider. Representation of meaning as a graph is only one of possible models, which assumes nodes and edges represent discrete entities. But meaning is continuous, and though a graph can be built on the base of discrete form (identifiers) but it is rather an interim form, which gives some approximation but is not complete.

What is meaning scope? You must remember meaning is always an abstraction. Even if you refer to a specific building as `home`, this word will remain to be abstract anyway, it could not encompass all atoms of a building, it could not tell the entire history of a building, it is just a reference to more or less the same building. On the other hand, `home` word by itself is a definition of, say, `a building where a person lives`. This definition is rather a type or a class of all homes we can imagine, and a scope of this definition is very wide. If we know a context (meaning, which summarizes or implicitly inherent to the previous discussion or description), then a scope can be narrowed but if a person has several houses (which are considered as homes) then it will stay to be a class for several things. The situation is even more complicated with abstract things. For instance, everyone will give slightly different definition of `logic` and even if words will be the same, different people can imply quite different meaning behind them. The exception is only some abstract things, which have defined equality rules like numbers. `5` is equal to `5` everywhere, but it won't be so once it is applied to other things: `5 apples` is equal to `5 apples` only with reservations.

That is, any abstraction defines vague area of applicability (meaning scope). In everyday life, we even don't notice when we switch from abstract definition to more specific meaning and vice versa because any abstraction is a balance between generalization and specificity. That's why any identifier based meaning representation could be precise only to some degree. And that's why we need meaning scope, which defines how many details are included into given meaning. For example, `Venus` identifier, a wiki page about Venus, and a book about Venus are about the same Venus but has different scope. Meaning of `Venus orbits` is not a graph with `Venus`, `orbits`, `planet` nodes but rather approximation of details, which are behind these identifiers (and are rather an intersection of two meanings for expressing object-action dualism).

Thus, to work with scopes we need a balanced approach to move between abstraction and specification, between finding similarities and differences. It should be applicable to any volume of data: you can refer to terabytes with one identifier, on the other hand, meaning in markup may have bigger volume than information itself. Such flexibility allows to describe both regular and irregular data, as well as not created yet data (by indicating its "pattern", etc). So theoretically you can apply it as many times as possible to reach acceptable level of approximation.

Additionally, comparing with modern semantic approaches like Semantic Web, this proposal:

* Does not imply usage of namely Web (and URIs, documents, resources in particular), this approach could be used for standalone information.
* Has slightly different understanding of meaning. It does not separate metadata and data, etc. Why `planet` is "metadata" but `Venus` is "data"? `Venus` is not less generalization than `planet`: the former is generalized by similarity in time, the latter is generalized by similarity of characteristics. If you would have `astronomical bodies` class, then `planet` (if no specific names mentioned) could be "data", couldn't? Similarly, `Venus` could be a value for a destination of planetary mission. The proposed approach considers everything as abstractions whereas "classes", "instances", "values" are rather "roles" of these abstractions.
* Implies human-readable and more flexible identification (through combinations of identifiers).
* Does not force to use specific models of data like triples and special storage types like triplestores. Instead, it is created to be complied with already existing data models.
* Does not imply a bunch of formats like XML, RDF, Turtle, OWL, SPARQL, RIF, etc because everything can be expressed with one. Clear boundary between triples and ontology is possible only for data, where such demarcation can be done by convention of parties. Separate query language is not required because any question is meaning too (with the only difference: they imply an unknown). Rules are not something specific from the point of view of meaning (and can be considered as conditions, etc).

Shift of paradigm
-----------------

The whole history of computer world is a history of machine-oriented approaches and technologies (including user interfaces). When we use computers or smartphones we unconsciously adjust our behavior for them. On one hand, to use a tool efficiently, we need to adapt ourselves for it. On the other hand, computer hardware provides infinite possibilities for user interfaces, so theoretically we can have interfaces, which will be adjusted for us and not vice versa. Ideal user interface (as it seems now) is natural language and possibly thought-driven (but it is not clear if it would be created). But even if computers will be able to recognize natural language, further transformation to computer entities will be required anyway as inside computer we have no natural based computer entities. So again, a bridge is needed.

But such bridge (or rather balanced approach) is not enough. We need to shift our representations of how we operate with computers. It would be strange if natural language interface continued to work with "Save a file in /home/user/me/newdooc.txt". Why do natural language statements need computer-specific elements like file names? We should deal not with files, resources, devices but with meaning. We have to work with information without knowing where it is stored and how to retrieve it. We have to interact with functionality without remembering long GUI paths to controls. It is nonsense when we write some document (with meaning inside), name it (thus, summarizing this meaning), put it into some specific folder (and attribute some meaning again), then forgot about its existence and location, and, finally, try to find it in a heap of many other documents. And all this because we encode meaning into data (at the best), possibly with some metadata, or to text, or to ambiguous abbreviations (at the worst), but we could not decode it back to more or less the same meaning without distortions. Semantic markup gives us a chance to use meaning on all levels without transformations. As for user interfaces, it implies that "where is information?" and "what is information?" and even "how to use this information?" questions can have the same answer.

Imagine we use `AplanetZ` application (of `AstroCo` company), where we can use `Show position` operation, which shows relative position of planets in a planetary system. Also imagine you checked right ascension and declination of Venus registered before (saved in `/home/me/doc/astro/venus/coord` file) and found this operation does not work correctly. You want to submit the bug but you don't know how yet, so decided to save all information in `/home/me/doc/astro/aplanetz/bug` file (or `/home/me/astro/app/apz/show_curr_pos/New Text DocumentAAA.txt`). These file paths demonstrates we save information in a folder hierarchy, where each level corresponds to some level of abstraction (which you consider reasonable on the moment of creation). Also it demonstrates we use somewhat arbitrary and usually abbreviated names. All this diminish our chances to find this information later because several months later we could think in different terms of abstraction.

Even, if we tag this file with `AstroCo AplanetZ Show position bug` or even with `AstroCo`, `AplanetZ`, `Show position`, and `bug` tags, the situation will be better only under certain conditions. First, you have to be sure the same identifiers will be used (for example, in old version this operation might be called `Display state`). Second, a number of used tags should be not big (realize search in a list of thousands names). Third, relations between names should not be ambiguous (for example, this bug may relate directly to the concerned operation, or occur after it, or caused by it, etc).

What does meaning change? `AstroCo {has} AplanetZ {has} Show position {has} bug` is similar to a directory path but differs because: (a) all words are meaningful and can be interpreted, (b) they are linked with relations, which clarifies their correspondence between each other, (c) it will imply more or less similar things for other users too, so, the entire meaning can be used after transfer (whereas on file copying sometimes only file name used). Separate words may imply that: (1) `AstroCo` is an instance of a company, (2) `AplanetZ` is an instance of an application and relates to astronomy, (3) `Show position` is an instance of an operation/action/function, and (4) `bug` is an instance of a bug/defect. In own turn, `Show position` means (3a) the application has a view, which imitates a planetary system, thus, this can be expressed through `planetary system {@} AplanetZ` meaning, which also implies this meaning `is similar` to real world planetary system, (3b) this view can be shown or hidden, so `show` action relates exclusively to the application and the view, (3c) `position` implies `position {of} planets {of} planetary system` but this is not real world position but rather `position {@} Aplanetz`, whereas real world position can be expressed with right ascension and declination (but don't forget these coordinates are relative to the Earth, so, they imply a subject too).

Why meaning is easier to find than text in files or with tags? Simple example: will `Earth mountains` query will return `Earth clouds are like mountains` as a result? Experience with search engines says yes. However, in terms of meaning `Earth {has} mountains` won't return `{#1} Earth {has} clouds {/#1 is similar #2} are like {#2} mountains {/#2}` because identifiers linked with different relations. Of course, this requires additional effort and we need to markup both query and destination. Of course, we all want it will be done automatically but as for now, we can have it working only manually. But even manual markup will pay off at least on restricted volumes of information. Unlike search through the whole Internet, you expect search on your own computer should be more precise, don't? But why this could be possible? Namely because we can operate with smaller units of information. For example, `Astronomy application {has part} display planets {^} defect` will match the original meaning because (1) `Astronomy application` as a type will match `AplanetZ` instance, (2) `has part` will match `has`, (3) `Display planets` will match `Show position` because, in own turn, `display` matches `show`, and `position` is unwinded as `position {of} planets {of} planetary system`, (4) `^` matches any relation, (5) `defect` matches `bug` as synonyms. That is, subsearches in smaller scopes are more probable to be precise.

How will this affect user intreface? Realize that each identifier(s) has a scope, which includes all related meaning (not only information itself but also elements of user interface). It is similar to file directories but without strict hierarchy, rather as vague overlapping circles of content. Your computer may have `AstroCo` scope/context, which may include `AplanetZ` scope/context, which, in own turn, may include `Show position` scope/context. Scopes can be referential only (for example, `AstroCo` context may include only one entry for similarity with `company` and one entry to include `AplanetZ` application, if you don't need any information about it) or may help to activate related function of an application and to contain information, which relates to this function.

When you need to create a piece of information, then you can use already existing context and just create new scope/context of, say, `bug` inside it. In result, you will have `AstroCo {has} AplanetZ {has} Show position {has} bug` scope/context, which may include description of the bug (`{specifies} Venus position is incorrect. Compare its right ascension and declination calculated in ... application and in AplanetZ.`) where `right ascension and declination calculated in ... application` may `specify` data saved by you before. Now you may select some information from this scope (that is, to create an abstract of your information as some parts of it cannot be duplicated, are unrelated or private) and transfer it to the application engineers through any available communication tool. On the other side of communication, this information may be automatically re-classified to match other people meaning structure. For example, engineers may have different base "hierarchy" of meaning (eg, `AstroCo {has} Planetary system module {has} Planetary system position`) but they also may have equivalency rules between structure of end-user application and internal organization of work. And if precise equivalency cannot be established, then some steps can be done manually, say, through dialog: "Are you sure `Show position` matches `Planetary system position`?", etc.

In some sense, the process described above is very similar to what you do with directories and files nowadays. But there are several important distinctions: (1) meaning is not lost after some time though could have not instant correspondence on other side of communication, (2) meaning can be reused (so you won't need to recreate the entire chain of identifiers and relations, and if another user accepts your variant, then it requires zero effort on his or her side), (3) separate consideration of identifiers and relations improves flexibility as we can operate with each element independently.

Meaningful everything
---------------------

Modern semantic and not semantic approaches miss one important point: everything is meaningful. Including computer entities. Files are ad-hoc division of meaning. File names should indicate an abstract of content (but often they don't). File paths should help to classify files (but ofthen they fail). What's about user interface? Usually it is not the focus of semantic theories but can we separate it from meaning itself? Isn't true complexity of the most semantic technologies partially may be attributed namely to the fact they ignore the problem of user interface? What is user interface from the point of view of meaning? If we look deeply, it is a set of tools, which helps to reduce, filter, order, and map meaning as well as to navigate through it, search, compose, etc. So, in some sense, user interface imitates cognition and is a part of meaning by itself.

What goals does user interface try to reach through its history? Command-line interface represents simplified version of natural language with identifiers, which coincide or do not with ones of natural language and which usually do not use grammar rules of it. By this, we can have much simpler parsing of commands and we are able to compose quite complex "sentences" of commands and "texts" (scripts). Evident drawback of such approach is you need to remember translation of computer commands to meaning and back. That's why graphical user interface appeared, which restricts meaning scope with windows, represents all related characteristics and values with controls and with natural language. Thus, the problem of translation was resolved (as everything is on screen) but, in own turn, we lost ability to compose complex "sentences" of commands. Another shortcoming (which is the consequence of imposed restrictions in terms of windows and controls) is to reach certain scope you need to remember long GUI "path" of clicks-opens-selects. Finally, Web-based user interface theoretically can solve problem of GUI paths by hyperreference "shortcuts" (but only when it does not imitate GUI) but still has problem with composability of command "sentences".

Meaningful user interface may try to solve stated above problems: (a) translation of computer commands are not required as meaning operates with natural language itself (though knowledge of small set of basic relations required), (b) meaning can have long paths too but as they are meaningful they are easily reachable. On the other hand, it may try to combine advantages of these interfaces: (a) complex "sentences" may be composed from meaning, text, commands, applications, GUI elements, (b) restrictions will be imposed with reducing, filtering, ordering, mapping, etc in terms of scopes, identifiers, types, properties, and other meaning elements, (c) it may operate with both ad-hoc scopes (like a GUI window or a web page) and flexible ones. Thus, you can imagine meaningful user interface as a sort of more versatile wiki page where each section (description, lists of parts/characteristics, classification, applicable actions, cause-effects, etc) if necessary may be extended/shrinked in terms of content.

What's about applications in terms of meaning? Long ago, in the era of command-line interface, they were interpreted as rather something that "accepts inputs and transforms them into output". Nowadays they are usually described as something that "performs a specific task when executed by a computer". The latter is rather machine-oriented description, whereas from the point of view of meaning the former definition is more appropriate. But true is rather in the middle: that is, an application "accepts input meaning, performs its transforming by a computer, and outputs result in another meaning". Meaning here should be understood broadly: it is not only information in the form of letters and numbers but also images, audio, video, that is any other representation of information. Usually, semantic approaches handle only symbolic data but this is the apparent shortcoming. Similarly, as natural language may be translated into data language, it may be translated into "language" of images, schemes, audio, or video. One example of this are image maps in hypertext (though usually we prefer to refer to images with the help of a short abstract). Another one is references to video fragments at modern video services but capabilities of video references may be extended by having scene and character mapping and linkage with semantic markup. What transformations are possible in applications (according with the definition)? Though there are a lot of them, futher, we will focus only on those, which implies some unknown in input.

Questions we ask
----------------

Not only applications but also Internet search, SQL, and Semantic Web under the guise of SPARQL operate with questions. But only with some. Search imitates ability to understand any type of questions but it is not really so. If you compare results with or without question words, you can easily notice they are more or less the same. Another trick of search engines is to use synonyms of question words (for example, consider `why` as a synonym of `cause`) to find relevancy but it gives statistically good results only for popular enough information. Though search advertisement claims it can use superlatives and space-time relations but it is true only for popular entities from the advertisement but does not work for slightly different queries. Try `What is the longest song of ...` (but do not choose popular bands because there could be exact pages with such title) or `Who was British PM when ... was founded` (try `The Beatles`). Unfortunately, in this case results returned for pages, which are relevant to query words, though we expect one identifier, which is result of inference for a complex of identifiers and relations.

Can the proposed approach help to answer to any question? Will be answers more precise? Imagine, you have a planet table in HTML or CSV file with data, which specific for your research. Also it could happen they will be required only once. This task is not for Internet search, which could not work correctly with tabular data. And it is not for regular applications just because it is not worthy to write an application (and sometimes even spreadsheet scripts) for one-time task. Whereas the markup can be easily applied as it requires to indicate only (a) the entire table relates to planets, (b) each column relates to certain property, and (c) each row relates to certain planet (applied only once by defining that each row is `{is instance of} planet`). Effort is really small but the effect is meaningful data, which may be queried with questions like `What is diameter of Jupiter?`, `When was Uranus discovered?`, `What is bigger: Uranus or Neptune?`, etc.

That is, we have a sort of database, but which is more meaningful than plain text or HTML and which is simpler to handle than database. But how does it compare against database itself? `What is diameter of Jupiter?` question is similar to SQL query like `SELECT diameter FROM planets WHERE name = 'Jupiter'`. But key differences are (1) all database model identifiers may be only partially meaningful for people (eg `planet_t` or `PLNT` which are not meaningful enough) and completely senseless for computer systems (as they are even not considered as meaning), (2) in most cases we cannot deduct automatically meaning of database "metadata", (3) database schema is not easily extensible (because it is optimized for quick data retrieval and its extending implies performance heavy operations), (4) a set of database "questions" is restricted (as databases can handle only questions about "properties", though, which, in own turn, could express different structural, behavioral, spatial, and temporal relations), (5) databases imply regular structure, whereas meaning may have free form.

How does meaning help to answer all types of questions? Question answering is rather comparing of meaning of question against available meaning. The only difference is unknown(s), which implied in any question. For markup we can use "unknown" identifier (`_`) or possibly "unknown variables" (like `_var`) with distinguishing what it belongs to (person, thing, action, space, time, etc). Quite probably it could be done automatically as a number of question words is restricted. Thus, mentioned question may look like `What {_ is value of #1} is {#1} diameter {/#1 is property of #2} of {#2} Jupiter {/#2}?`, which will match meaning with similar structure: `Jupiter {has} diameter {has value} 142984 km`. In this simple example, matching is possible because (a) `Jupiter` and `diameter` match themselves and `has` may match `has property` as it is the generalizing version of it, (b) unknown value matches `142984 km` as both linked with the same relation (`is value of`\`has value`).

But what's if an answer cannot be found at your computer? It would be not efficient to use regular search for that. Instead, we need rather meaning querying system (similarly to DNS), which will delegate your question or its parts to other servers (or maybe even just to other applications), which can answer. For example, `What is radius of the biggest planet is the Solar System?` can be answered by before mentioned planet table but we have no `radius` property and `the biggest planet in the Solar System` is unknown for us but can be delegated to different servers/applications: when the former will be answered with a formula or with a fact of dependency on `diameter` and the latter with `Jupiter` value, the final answer is clear.

That's not all. More complex questions and answers will follow. For instance, let us consider simple math problem: `Mary has 5 apples, and Tom gave her 7 apples, then Mary ate 3 apples. How many apples Mary has now?`. How can this be answered?

1. `Mary has 5 apples` denotes (a) `Mary` is an identifier (for this task we can ignore what `Mary` really means, for us it is enough to know `Mary` can have apples), (b) `Mary {has} 5 apples`, where `{has}` is undefined `has` relation, (c) `5` is an identifier for certain number, which has `{units of} apples`. It may be expressed like `{#m} Mary {/#m} {has} 5 {units of} apples`.
2. `Tom gave her 7 apples` denotes (a) `Tom` is an identifier for someone or something (it does not matter here) which may give apples, (b) `gave ... apples` is equivalent to mathematical operation of addition, which changes `{has}` relation between `her` and `apples`, (c) `her` is an identifier of reference to a female one (here it is `Mary`). Which produces `{#t} Tom {/#t} gave {has property #t1 #t2} {is the same addition} {#t1} her {/#t1} {is the same #m} {#t2} 7 {/#t2} {units of} apples`.
3. `Mary ate 3 apples` denotes (a) the same `Mary` as in the first statement (this is assumed by default), (b) `ate` is equivalent to mathematical operation of subtraction. Which produces `{#mm} Mary {/#mm} {is the same #m} ate {has property #mm1} {is the same subtraction} {#mm1} 3 {#mm1} {units of} apples`.
4. Further, we need to arrange everything in space-time: item 2 occurs after item 1 and item 3 occurs after item 2. Which may be expressed (for simplicity the previously mentioned markup is omitted): `{#ev1} Mary has 5 apples {/#ev1 then #ev2}, and {#ev2} Tom gave her 7 apples {/#ev2 then #ev3}, then {#ev3} Mary ate 3 apples {/ev3}.`
5. All three items modifies `{has}` relation between `Mary` and `apples`:
  * At the first step we link `Mary` with `{has}` relation and `5` `units of` `apples`.
  * At the second one we add `7` to `5` as an owner of them and units are the same.
  * At the third one we reduce it by `3` by the same reasoning.
6. As the final question is about `Mary` and `{has}` relation with `apples`, the answer is `9`.

Conclusion
----------

Meaning can change our lives if will be understood appropriately, and widely used in simple and intuitive way. In fact, many things discussed above are implied by search engines and other semantic and not quite semantic (or not called so) technologies. But they are used not adequately enough, which is clear from a lot of minor details. Why does `Venus` has one main entity in search engines and 50+ disambiguations in wiki? Why search engines still cannot recognize different entities which are behind the same identifier? There is thin understanding in the air that human readable identifiers like `Venus goddess` may give better results but why is this not stated explicitly? Why are such identifiers not prioritized in suggestions? Why, in the name of Venus, there are a lot of pages with duplicate information with the same facts? Can't a search engine summarize or group them? Why do search suggestions for `Venus` include different entities (planet, goddess, movies) without clear boundaries? Can you imagine a lot of situations when you looked for Venus (planet) but then switched to Venus (movie)? Well, we can. Out of blind curiosity or boredom but usually our intentions are more conscious.

Meaning gives us a chance for changes. We should think broader and get rid of bonds of machine dependency. Our interaction with computer should turn around humans not computers. Really intuitive user interface should not make you think how to do what you want but just do what you want. In own turn, this implies more active usage of information than it is now. But here is a catch: you will need to design, develop, and test sources of meaning. Yes, even for a small page with planet data. This is inevitable step when meaning will receive widespread support. Today, it is responsibility of software engineering to transform meaning of certain domain to applications, which, in own turn, provides meaningful representations back to users. But with meaning markup the same will be applied to any source of information.

As you can see, the balanced approach implies not only simplification but also additional effort for information, which is "just written" today. But a balance is always necessity to move into opposite directions, in our case, it is bottom-up and top-down. Modern semantic approaches are rather top-down ones, therefore we must to focus on bottom-up direction. We must learn to think about meaning in really simple terms and apply the simplest possible approaches (but not simpler than necessary). And only after this, we can turn to something more complex. Namely therefore, we propose to use restricted set of relations and lightweight hierarchy of them, which allows to use relations that fit _your_ level of understanding. That's why we accentuate that identification is a combination of both generalization and specification, both finding similarities and differences, both composability and characteristics of things and concepts. Simply put, identification is the essence of the whole semantics. It is expressed with different hues of meaning but, finally, any Big Data with thousands of relations, fields, and classes is identification of something in space and time, real or abstract.

[Meaningful.js](https://github.com/meaningfuljs/meaningfuljs) lightweight experimental library (which text size is about 80KB) demonstrates some points of this article (though with slightly different syntax). It allows to answer simple questions and solve not complex mathematical problems. Of course, it won't be lightweight if everything described in this article will be implemented. Of course, it lacks many things but at least it proves conception of simplified meaning markup. This is the first small step but we have to take many other ones to accept and propagate wider usage of meaning. We simply have no choice: this way or another but we will always work with generalized identifiers and specific computer data (at least in scope of the current computer architecture). And we will always work between more or less satisfactory consistency and more or less satisfactory completeness. We are to balance between them. We have to face this problem. And if you at least started to think in this direction, then the goal of the article is achieved.